general:
  project_name: "my-llama-finetune"
  seed: 42
  output_dir: "./outputs"
  push_to_hub: false
  hub_model_id: null
  deploy_after_training: false

dataset:
  name_or_path: "./data/my_dataset.csv"
  text_column: "text"
  validation_split_percentage: 5
    # Option A: merge multiple columns (e.g. instruction, input, output)
  text_columns:
    - instruction
    - input
    - output


  text_column: none                

  # If neither text_columns nor text_column is set,
  # the code will auto-detect:
  # 1) a `text` column, or
  # 2) all of `instruction`, `input`, `output`, or
  # 3) any string-typed columns in your data

  tokenizer_name: "meta-llama/Llama-2-7b-hf"
  max_length: null
  cache_dir: null

model:
  base_model: "meta-llama/Llama-2-7b-hf"
  load_in_4bit: true
  load_in_8bit: false
  compute_dtype: "bfloat16"
  quant_type: "nf4"
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  lora_bias: "none"
  lora_task_type: "CAUSAL_LM"

training:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  max_steps: 1000
  learning_rate: 2e-5
  lr_scheduler_type: "cosine"
  warmup_steps: 50
  bf16: true
  fp16: false
  logging_steps: 10
  save_steps: 200
  eval_steps: 200

logging:
  use_tensorboard: true
  tb_log_dir: "./runs"

wandb:
  use_wandb: true
  project: "llama-playground"
  entity: "my-team"

resources:
  auto_check: true
  vram_margin_mb: 1024
  dummy_seq_len: 512

deployment:
  type: "none"
  gradio_port: 7860

inference:
  generation_kwargs:
    max_new_tokens: 128
    temperature: 0.7
    top_k: 50
    top_p: 0.9
